# Research Pipeline Progress Checklist

Track your progress through the research pipeline phases.

## ‚úÖ Foundation (Completed - Nov 2, 2025)

### Phase 1: Setup
- [x] Evaluation rubric (20 dimensions) ‚Üí `evaluation_rubric.md`
- [x] Project directory structure
- [x] Cache manager system
- [x] Pipeline logger system
- [x] Configuration template ‚Üí `config.yaml`

### Phase 2: Data Preprocessing
- [x] Dataset curation script ‚Üí `scripts/curate_dataset.py`
- [x] Interactive sample adder ‚Üí `scripts/add_samples_interactive.py`
- [x] Preprocessing script ‚Üí `scripts/preprocess_samples.py`
- [x] Dataset guide ‚Üí `DATASET_GUIDE.md`

### Phase 3: Prompt Templates
- [x] Critique template ‚Üí `prompts/critique_template.txt`
- [x] Improvement template ‚Üí `prompts/improve_template.txt`
- [x] Re-critique template ‚Üí `prompts/recritique_template.txt`
- [x] Naming critique template ‚Üí `prompts/naming_critique_template.txt`

### Phase 4: Pipeline Core
- [x] LLM integration layer ‚Üí `code_wise/pipeline/model_api.py`
- [x] Sample processor ‚Üí `code_wise/pipeline/sample_processor.py`
- [x] Batch processor ‚Üí `code_wise/pipeline/batch_processor.py`
- [x] CLI interface ‚Üí `--help` in batch_processor.py

### Documentation
- [x] Technical documentation ‚Üí `PIPELINE.md`
- [x] Quick start guide ‚Üí `QUICKSTART.md`
- [x] Implementation summary ‚Üí `IMPLEMENTATION_SUMMARY.md`
- [x] This checklist ‚Üí `PROGRESS_CHECKLIST.md`

---

## üîÑ In Progress - Data Curation & Testing

### Phase 2.1: Curate Dataset

**Goal**: Collect 30-70 code samples

- [ ] **Week 1**: Collect initial 15-20 samples
  - [ ] Open-source samples (GitHub)
    - [ ] 5 utility functions
    - [ ] 5 data processing functions
    - [ ] 5 web/API functions
  - [ ] LLM-generated samples
    - [ ] 5 generated by Claude
    - [ ] 5 generated by GPT-4
  - [ ] Intentionally bad samples
    - [ ] 3-5 with known issues

  **Tracking**:
  ```bash
  python -c "from scripts.curate_dataset import DatasetCurator; c = DatasetCurator(); print(f'Samples: {c.get_sample_count()}')"
  ```

- [ ] **Week 2**: Expand to 30-50 samples
  - [ ] Add 10-15 more diverse samples
  - [ ] Review metadata for completeness

- [ ] **Week 3**: Final push to 50-70 samples
  - [ ] Fill gaps in coverage
  - [ ] Validate all samples

**Acceptance Criteria**:
- [ ] 30-70 samples in `datasets/original_code/`
- [ ] Metadata CSV complete with all columns
- [ ] All samples have valid Python syntax
- [ ] Good distribution across categories and qualities

### Phase 2.2: Preprocess All Samples

- [ ] Run preprocessing script
  ```bash
  python scripts/preprocess_samples.py
  ```

- [ ] Check output
  ```bash
  cat intermediate/parsed_code_metadata.json | python -m json.tool | head -50
  ```

- [ ] Fix any syntax errors identified
- [ ] Verify metadata extraction worked

**Acceptance Criteria**:
- [ ] `intermediate/parsed_code_metadata.json` generated
- [ ] All samples parsed successfully
- [ ] AST metadata extracted for all samples

---

## üìä Phase 5: Analysis & Evaluation (TODO)

### Phase 5.1: Score Comparison Analysis

- [ ] Create analysis module
  - [ ] Load all critique outputs
  - [ ] Extract scores from each model
  - [ ] Compare scores across models
  - [ ] Generate comparison CSV

- [ ] Output files:
  - [ ] `outputs/analysis/score_comparison.csv`
  - [ ] `outputs/analysis/model_agreement.json`
  - [ ] `outputs/analysis/score_statistics.json`

**Metrics to calculate**:
- [ ] Mean score by model
- [ ] Standard deviation by model
- [ ] Model agreement/correlation
- [ ] Score improvement distribution
- [ ] Dimension-wise comparison

### Phase 5.2: Naming Quality Analysis

- [ ] Set up embeddings
  - [ ] Choose embedding model (CodeBERT or similar)
  - [ ] Generate embeddings for variable names

- [ ] Analyze naming quality
  - [ ] Extract variable names from original code
  - [ ] Extract variable names from improved code
  - [ ] Calculate semantic similarity
  - [ ] Compare with LLM feedback

- [ ] Output files:
  - [ ] `outputs/analysis/naming_improvements.json`
  - [ ] `outputs/analysis/naming_heatmap.html`

### Phase 5.3: Feedback Quality Assessment

- [ ] Measure critique actionability
  - [ ] Parse feedback for specific suggestions
  - [ ] Count actionability score

- [ ] Measure consistency
  - [ ] Compare feedback across similar samples
  - [ ] Calculate consistency score

- [ ] Output files:
  - [ ] `outputs/analysis/feedback_quality.json`
  - [ ] `outputs/analysis/consistency_report.json`

### Phase 5.4: Visualizations & Reports

- [ ] Create visualizations
  - [ ] Score distribution plots
  - [ ] Model comparison heatmaps
  - [ ] Score improvement scatter plots
  - [ ] Naming improvement trends

- [ ] Generate report
  - [ ] Executive summary
  - [ ] Key findings
  - [ ] Model-by-model analysis
  - [ ] Recommendations

- [ ] Output files:
  - [ ] `outputs/analysis/visualizations.html`
  - [ ] `outputs/analysis/executive_summary.md`
  - [ ] `outputs/analysis/detailed_report.html`

---

## üß™ Phase 6: Validation & Iteration (TODO)

### Phase 6.1: Manual Review

- [ ] Review sample outputs (10-20 samples)
  - [ ] [ ] Check critique accuracy
  - [ ] [ ] Validate improvement quality
  - [ ] [ ] Assess feedback usefulness

- [ ] Documentation
  - [ ] Findings document
  - [ ] Issues log
  - [ ] Prompt refinements needed

### Phase 6.2: Gemma Integration (Optional)

- [ ] Set up Gemma
  - [ ] [ ] Install Ollama
  - [ ] [ ] Pull gemma model
  - [ ] [ ] Test connection

- [ ] Implement GemmaReviewer
  - [ ] [ ] Complete model API implementation
  - [ ] [ ] Test critique functionality
  - [ ] [ ] Test improve functionality
  - [ ] [ ] Test recritique functionality

- [ ] Run comparison
  - [ ] [ ] Process 10 samples with Gemma
  - [ ] [ ] Add to comparison analysis

### Phase 6.3: Statistical Validation

- [ ] Calculate statistics
  - [ ] [ ] Inter-rater reliability (Cronbach's alpha, Fleiss' kappa)
  - [ ] [ ] Correlation analysis
  - [ ] [ ] Outlier detection

- [ ] Document findings
  - [ ] Statistical summary
  - [ ] Limitations discussion
  - [ ] Confidence intervals

- [ ] Output files:
  - [ ] `outputs/analysis/statistics.json`
  - [ ] `outputs/analysis/statistical_report.md`

---

## üìù Phase 7: Documentation & Reporting (TODO)

### Phase 7.1: Code Documentation

- [ ] Docstrings
  - [ ] [ ] Add comprehensive docstrings to all modules
  - [ ] [ ] Document all classes
  - [ ] [ ] Document all public methods

- [ ] Architecture docs
  - [ ] [ ] Component diagrams
  - [ ] [ ] Data flow diagrams
  - [ ] [ ] Update PIPELINE.md with latest info

- [ ] Extension guide
  - [ ] [ ] How to add new models
  - [ ] [ ] How to modify rubric
  - [ ] [ ] How to customize prompts

### Phase 7.2: Final Research Report

- [ ] Introduction
  - [ ] [ ] Research questions
  - [ ] [ ] Hypothesis
  - [ ] [ ] Significance

- [ ] Methodology
  - [ ] [ ] Dataset description
  - [ ] [ ] Evaluation approach
  - [ ] [ ] Models tested

- [ ] Results
  - [ ] [ ] Score comparisons
  - [ ] [ ] Model agreement metrics
  - [ ] [ ] Improvement effectiveness
  - [ ] [ ] Naming quality findings

- [ ] Discussion
  - [ ] [ ] Model strengths/weaknesses
  - [ ] [ ] Implications
  - [ ] [ ] Limitations

- [ ] Conclusion & Future Work
  - [ ] [ ] Key takeaways
  - [ ] [ ] Recommendations
  - [ ] [ ] Next steps

- [ ] Deliverables
  - [ ] [ ] Research paper (5,000+ words)
  - [ ] [ ] Supporting data/charts
  - [ ] [ ] Code repository with documentation

---

## üöÄ Getting Started (Next Steps)

### This Week (Immediate)

1. **Review Documentation** (1 hour)
   - [ ] Read `QUICKSTART.md`
   - [ ] Read `DATASET_GUIDE.md`
   - [ ] Review `config.yaml`

2. **Add First Samples** (1-2 hours)
   - [ ] Run interactive curator
   - [ ] Add 5-10 test samples
   - [ ] Verify they appear in `datasets/original_code/`

3. **Preprocess Samples** (30 minutes)
   - [ ] Run `python scripts/preprocess_samples.py`
   - [ ] Check `intermediate/parsed_code_metadata.json`
   - [ ] Verify all samples processed

4. **Test Pipeline** (30 minutes)
   - [ ] Dry run: `python -m code_wise.pipeline.batch_processor --dry-run --max-samples 2`
   - [ ] Real run: `python -m code_wise.pipeline.batch_processor --max-samples 2`
   - [ ] Check outputs in `outputs/{claude,gpt4}/`

**Time estimate**: 3-4 hours
**Cost**: $0.50-1.00

### Next 2 Weeks (Scale Up)

5. **Expand Dataset** (4-6 hours)
   - [ ] Add 20-30 more samples
   - [ ] Aim for 30-50 total

6. **Run Full Pipeline** (2-3 hours active time, 4-8 hours API processing)
   - [ ] Run on all samples: `python -m code_wise.pipeline.batch_processor --resume`
   - [ ] Monitor progress
   - [ ] Check outputs

**Time estimate**: 6-9 hours of work
**Cost**: $15-25 (estimated for 30-50 samples)

---

## üìã Tracking Template

Copy this to track weekly progress:

```markdown
## Week X Progress

**Samples added**: __ / 70
**Samples processed**: __ / total
**Analysis done**: Yes / No

### Completed
- [ ] ...
- [ ] ...

### In Progress
- [ ] ...
- [ ] ...

### Blocked
- [ ] ...
- [ ] ...

### Notes
...
```

---

## üí° Tips for Success

1. **Start small**: Test with 2-3 samples before full dataset
2. **Use dry-run**: Always test with `--dry-run` first
3. **Monitor costs**: Check `logs/api_calls.jsonl` frequently
4. **Save often**: Use `--resume` to avoid reprocessing
5. **Document findings**: Keep notes on what works/doesn't
6. **Iterate quickly**: Refine prompts based on early results

---

## üìû Getting Help

**If you get stuck**:

1. Check the relevant documentation:
   - General pipeline issues ‚Üí `PIPELINE.md`
   - Dataset questions ‚Üí `DATASET_GUIDE.md`
   - Getting started ‚Üí `QUICKSTART.md`

2. Debug using logs:
   ```bash
   tail -f logs/pipeline.log
   tail -f logs/api_calls.jsonl
   ```

3. Check code comments for detailed explanations

4. Verify inputs:
   - Samples in `datasets/original_code/`?
   - API keys in `.env`?
   - Dependencies installed with `uv pip install`?

---

## üéØ Success Criteria

### MVP (Phases 1-4): ‚úÖ COMPLETE
- [x] Rubric designed
- [x] Pipeline built
- [x] Prompts created
- [x] 2 models integrated

### Full Implementation (Phases 5-7): TODO
- [ ] Score comparison analysis
- [ ] Naming quality analysis
- [ ] Statistical validation
- [ ] Research report written

### Publication Ready (Stretch)
- [ ] Code published to GitHub
- [ ] Documentation complete
- [ ] Results validated by peers
- [ ] Paper submitted to conference/journal

---

## Last Updated

- **Created**: November 2, 2025
- **Last Updated**: November 2, 2025
- **Next Review**: When you start Phase 5

Good luck! üéì
